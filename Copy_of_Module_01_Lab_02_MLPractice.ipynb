{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kongarapukaveri/FMML-LAB1/blob/main/Copy_of_Module_01_Lab_02_MLPractice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Eu9VZbF01eq"
      },
      "source": [
        "# Machine learning terms and metrics\n",
        "\n",
        "FMML Module 1, Lab 2<br>\n",
        "\n",
        "\n",
        " In this lab, we will show a part of the ML pipeline by extracting features, training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qBvyEem0vLi"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "# set randomseed\n",
        "rng = np.random.default_rng(seed=42)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3t59g5s1HfC"
      },
      "source": [
        "In this lab, we will use the California Housing dataset. There are 20640 samples, each with 8 attributes like income of the block, age of the houses per district etc. The task is to predict the cost of the houses per district.\n",
        "\n",
        "Let us download and examine the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LpqjN991GGJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "682d35b2-624e-4146-a740-fc04fd47ccde"
      },
      "source": [
        " dataset =  datasets.fetch_california_housing()\n",
        " # print(dataset.DESCR)  # uncomment this if you want to know more about this dataset\n",
        " # print(dataset.keys())  # if you want to know what else is there in this dataset\n",
        " dataset.target = dataset.target.astype(np.int) # so that we can classify\n",
        " print(dataset.data.shape)\n",
        " print(dataset.target.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20640, 8)\n",
            "(20640,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-60ae2e9a125e>:4: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  dataset.target = dataset.target.astype(np.int) # so that we can classify\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNx4174W5xRg"
      },
      "source": [
        "Here is a function for calculating the 1-nearest neighbours"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07zpydQj1hIQ"
      },
      "source": [
        "def NN1(traindata, trainlabel, query):\n",
        "  diff  = traindata - query  # find the difference between features. Numpy automatically takes care of the size here\n",
        "  sq = diff*diff # square the differences\n",
        "  dist = sq.sum(1) # add up the squares\n",
        "  label = trainlabel[np.argmin(dist)] # our predicted label is the label of the training data which has the least distance from the query\n",
        "  return label\n",
        "\n",
        "def NN(traindata, trainlabel, testdata):\n",
        "  # we will run nearest neighbour for each sample in the test data\n",
        "  # and collect the predicted classes in an array using list comprehension\n",
        "  predlabel = np.array([NN1(traindata, trainlabel, i) for i in testdata])\n",
        "  return predlabel"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03JktkfIGaje"
      },
      "source": [
        "We will also define a 'random classifier', which randomly allots labels to each sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fogWAtjyGhAH"
      },
      "source": [
        "def RandomClassifier(traindata, trainlabel, testdata):\n",
        "  # in reality, we don't need these arguments\n",
        "\n",
        "  classes = np.unique(trainlabel)\n",
        "  rints = rng.integers(low=0, high=len(classes), size=len(testdata))\n",
        "  predlabel = classes[rints]\n",
        "  return predlabel"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hjf1KHs7fU5"
      },
      "source": [
        "Let us define a metric 'Accuracy' to see how good our learning algorithm is. Accuracy is the ratio of the number of correctly classified samples to the total number of samples. The higher the accuracy, the better the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouuCqWU07bz-"
      },
      "source": [
        "def Accuracy(gtlabel, predlabel):\n",
        "  assert len(gtlabel)==len(predlabel), \"Length of the groundtruth labels and predicted labels should be the same\"\n",
        "  correct = (gtlabel==predlabel).sum() # count the number of times the groundtruth label is equal to the predicted label.\n",
        "  return correct/len(gtlabel)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vJFwBFa9Klw"
      },
      "source": [
        "Let us make a function to split the dataset with the desired probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko0VzpSM2Tdi"
      },
      "source": [
        "def split(data, label, percent):\n",
        "  # generate a random number for each sample\n",
        "  rnd = rng.random(len(label))\n",
        "  split1 = rnd<percent\n",
        "  split2 = rnd>=percent\n",
        "  split1data = data[split1,:]\n",
        "  split1label = label[split1]\n",
        "  split2data = data[split2,:]\n",
        "  split2label = label[split2]\n",
        "  return split1data, split1label, split2data, split2label"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcK3LEAJ_LGC"
      },
      "source": [
        "We will reserve 20% of our dataset as the test set. We will not change this portion throughout our experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBZkHBLJ1iU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99210a7c-8f13-490a-d1ba-27b74c717683"
      },
      "source": [
        "testdata, testlabel, alltraindata, alltrainlabel = split(dataset.data, dataset.target, 20/100)\n",
        "print('Number of test samples = ', len(testlabel))\n",
        "print('Number of other samples = ', len(alltrainlabel))\n",
        "print('Percent of test data = ', len(testlabel)*100/len(dataset.target),'%')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test samples =  4144\n",
            "Number of other samples =  16496\n",
            "Percent of test data =  20.07751937984496 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6Ss0Z6IAGNV"
      },
      "source": [
        "## Experiments with splits\n",
        "\n",
        "Let us reserve some of our train data as a validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFew2iry_7W7"
      },
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 75/100)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60hiu4clFN1i"
      },
      "source": [
        "What is the accuracy of our classifiers on the train dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBlZDTHUFTZx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8177584-e847-4356-a677-5f4aea3c0f78"
      },
      "source": [
        "trainpred = NN(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Train accuracy using nearest neighbour is \", trainAccuracy)\n",
        "\n",
        "trainpred = RandomClassifier(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Train accuracy using random classifier is \", trainAccuracy)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy using nearest neighbour is  1.0\n",
            "Train accuracy using random classifier is  0.164375808538163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h08-9gJDtSy"
      },
      "source": [
        "For nearest neighbour, the train accuracy is always 1. The accuracy of the random classifier is close to 1/(number of classes) which is 0.1666 in our case.\n",
        "\n",
        "Let us predict the labels for our validation set and get the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h7bXoW_2H3v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bf438ca-bf35-4c16-f9ce-ad7925364efb"
      },
      "source": [
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using nearest neighbour is \", valAccuracy)\n",
        "\n",
        "valpred = RandomClassifier(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using random classifier is \", valAccuracy)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy using nearest neighbour is  0.34108527131782945\n",
            "Validation accuracy using random classifier is  0.1688468992248062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py9bLguFEjfg"
      },
      "source": [
        "Validation accuracy of nearest neighbour is considerably less than its train accuracy while the validation accuracy of random classifier is the same. However, the validation accuracy of nearest neighbour is twice that of the random classifier.\n",
        "\n",
        "Now let us try another random split and check the validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujm3cyYzEntE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4088b3e-61e6-4ca3-e3ea-678ec908bdef"
      },
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 75/100)\n",
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy of nearest neighbour is \", valAccuracy)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy of nearest neighbour is  0.34048257372654156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSOx7U83EKie"
      },
      "source": [
        "You can run the above cell multiple times to try with different random splits.\n",
        "We notice that the accuracy is different for each run, but close together.\n",
        "\n",
        "Now let us compare it with the accuracy we get on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNEZ5ToYBEDW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a381ac9-7a43-417e-d4c6-13bb4258d6be"
      },
      "source": [
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "testAccuracy = Accuracy(testlabel, testpred)\n",
        "print('Test accuracy is ', testAccuracy)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy is  0.34917953667953666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3dGD531K3gH"
      },
      "source": [
        "### Try it out for yourself and answer:\n",
        "1. How is the accuracy of the validation set affected if we increase the percentage of validation set? What happens when we reduce it?\n",
        "2. How does the size of the train and validation set affect how well we can predict the accuracy on the test set using the validation set?\n",
        "3. What do you think is a good percentage to reserve for the validation set so that thest two factors are balanced?\n",
        "\n",
        "Answer for both nearest neighbour and random classifier. You can note down the values for your experiments and plot a graph using  <a href=https://matplotlib.org/stable/gallery/lines_bars_and_markers/step_demo.html#sphx-glr-gallery-lines-bars-and-markers-step-demo-py>plt.plot<href>. Check also for extreme values for splits, like 99.9% or 0.1%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnYvkAZLQY7h"
      },
      "source": [
        "## Multiple Splits\n",
        "\n",
        "One way to get more accurate estimates for the test accuracy is by using <b>crossvalidation</b>. Here, we will try a simple version, where we do multiple train/val splits and take the average of validation accuracies as the test accuracy estimation. Here is a function for doing this. Note that this function will take a long time to execute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4nGCUQXBTzo"
      },
      "source": [
        "# you can use this function for random classifier also\n",
        "def AverageAccuracy(alldata, alllabel, splitpercent, iterations, classifier=NN):\n",
        "  accuracy = 0\n",
        "  for ii in range(iterations):\n",
        "    traindata, trainlabel, valdata, vallabel = split(alldata, alllabel, splitpercent)\n",
        "    valpred = classifier(traindata, trainlabel, valdata)\n",
        "    accuracy += Accuracy(vallabel, valpred)\n",
        "  return accuracy/iterations # average of all accuracies"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3qtNar7Bbik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5d47017-f22d-4846-a840-9a2f08342ad0"
      },
      "source": [
        "print('Average validation accuracy is ', AverageAccuracy(alltraindata, alltrainlabel, 75/100, 10, classifier=NN))\n",
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "print('test accuracy is ',Accuracy(testlabel, testpred) )"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average validation accuracy is  0.33584635395170215\n",
            "test accuracy is  0.34917953667953666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33GIn4x5VH-d"
      },
      "source": [
        "This is a very simple way of doing cross-validation. There are many well-known algorithms for cross-validation, like k-fold cross-validation, leave-one-out etc. This will be covered in detail in a later module. For more information about cross-validation, check <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)>Cross-validatioin (Wikipedia)</a>\n",
        "\n",
        "### Questions\n",
        "1. Does averaging the validation accuracy across multiple splits give more consistent results?\n",
        "2. Does it give more accurate estimate of test accuracy?\n",
        "3. What is the effect of the number of iterations on the estimate? Do we get a better estimate with higher iterations?\n",
        "4. Consider the results you got for the previous questions. Can we deal with a very small train dataset or validation dataset by increasing the iterations?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUESTION **1**"
      ],
      "metadata": {
        "id": "Kk7oBb8oAxde"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, averaging validation accuracy across multiple splits can provide more consistent and robust results compared to relying on a single split. This practice is commonly known as cross-validation. Cross-validation involves splitting the dataset into multiple subsets, training the model on different subsets, and evaluating its performance on the remaining data. The results are then averaged to obtain a more reliable estimate of the model's performance.\n",
        "\n",
        "Here are a few reasons why cross-validation can lead to more consistent results:\n",
        "\n",
        "Reduced Dependency on a Single Split:\n",
        "\n",
        "In a single train-test split, the performance of the model may be highly dependent on the specific samples chosen for training and testing. Cross-validation helps reduce this dependency by using multiple splits, ensuring that the model is evaluated on different subsets of the data.\n",
        "Better Generalization:\n",
        "\n",
        "Cross-validation helps in assessing how well the model generalizes to different subsets of the data. If the model consistently performs well across multiple splits, it is likely to be more robust and have better generalization performance.\n",
        "More Reliable Performance Estimate:\n",
        "\n",
        "Averaging results from multiple splits provides a more stable estimate of the model's performance. This can be particularly important when dealing with limited data, as it helps mitigate the impact of data variability.\n",
        "Identifying Overfitting:\n",
        "\n",
        "Cross-validation can help detect overfitting. If a model performs well on the training data but poorly on unseen data in multiple splits, it may be a sign of overfitting.\n",
        "Common types of cross-validation include k-fold cross-validation and stratified k-fold cross-validation. In k-fold cross-validation, the dataset is divided into k subsets, and the model is trained and evaluated k times, with each subset serving as the test set exactly once. Stratified k-fold ensures that each fold maintains the same class distribution as the original dataset.\n",
        "\n",
        "Keep in mind that the choice of the number of folds (k) and the specific type of cross-validation may depend on factors such as the size of your dataset and the nature of your problem.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IdP0SfohA0JV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUESTION **2**"
      ],
      "metadata": {
        "id": "kYHwk617A21-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-validation, by providing an average performance metric over multiple folds, can indeed give a more accurate estimate of how well a model is likely to perform on unseen data compared to a single train-test split. However, it's important to clarify that the primary goal of cross-validation is to provide a more reliable estimate of the model's generalization performance on the training data. The term \"test accuracy\" is commonly associated with the performance on a separate test set that the model has not seen during training.\n",
        "\n",
        "Here's how cross-validation contributes to a more accurate estimate:\n",
        "\n",
        "Reduced Variability:\n",
        "\n",
        "Averaging the performance over multiple folds helps reduce the impact of randomness and variability that might be present in a single train-test split. This provides a more stable estimate of the model's performance.\n",
        "Better Representation of Data:\n",
        "\n",
        "Each fold in cross-validation represents a different subset of the data. By training and testing the model on different subsets, cross-validation helps ensure that the model's performance is representative of the entire dataset, capturing different patterns and variations.\n",
        "Mitigating Overfitting:\n",
        "\n",
        "Cross-validation allows you to assess how well the model generalizes across different subsets of the data. If the performance is consistent across folds, it suggests that the model is likely to generalize well to unseen data.\n",
        "While cross-validation provides a more accurate estimate of the model's generalization performance on the training data, it doesn't replace the need for a separate test set to evaluate the model's performance on truly unseen data. In practice, it's common to use cross-validation during the model development phase to tune hyperparameters and assess performance, and then reserve a final test set for the ultimate evaluation of the model's readiness for deployment.\n",
        "\n",
        "Remember that the accuracy estimate obtained through cross-validation is still an estimate based on the specific splits used, and the actual performance on new, unseen data may vary. It's always a good practice to use a final test set that the model has never encountered during training or cross-validation to provide a more realistic assessment of its performance in a real-world scenario.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TND744xjBInR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUESTION **3**"
      ],
      "metadata": {
        "id": "jSLQ9M5UBKIR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of iterations or folds in cross-validation can impact the estimate of the model's performance. The choice of the number of iterations is often referred to as \"k\" in k-fold cross-validation. Common values for k include 5, 10, or more, but the optimal choice can depend on factors such as the size of the dataset and the nature of the problem.\n",
        "\n",
        "Here's how the number of iterations can affect the estimate:\n",
        "\n",
        "More Stable Estimate with Higher k:\n",
        "\n",
        "As you increase the number of iterations (higher k), the cross-validation process becomes more robust, providing a more stable estimate of the model's performance. This is because the model is trained and evaluated on different subsets of the data, and averaging over more folds helps reduce the impact of variability.\n",
        "Computational Cost:\n",
        "\n",
        "A higher number of iterations generally lead to a higher computational cost since the model needs to be trained and evaluated multiple times. The trade-off between computational cost and the accuracy of the estimate should be considered when choosing the value of k.\n",
        "Smaller Test Set in Each Fold:\n",
        "\n",
        "When you have a limited amount of data, a higher value of k means that each fold has a smaller test set, which can reduce the reliability of the estimate. In such cases, it might be beneficial to use a smaller value of k.\n",
        "Balance with Dataset Size:\n",
        "\n",
        "The choice of k should be balanced with the size of your dataset. If you have a large dataset, you might be able to use a larger value of k without sacrificing the size of each test set. For smaller datasets, it's common to use smaller values of k.\n",
        "It's important to note that there are diminishing returns with increasing values of k. While higher values of k can provide a more stable estimate, the improvement in stability may become marginal beyond a certain point, and the computational cost may become prohibitive.\n",
        "\n",
        "In practice, it's often recommended to start with a moderate value of k (e.g., 5 or 10) and then adjust based on considerations such as the size of the dataset and available computational resources. Cross-validation is a valuable tool for model evaluation and hyperparameter tuning, but it's crucial to complement it with a final evaluation on a separate test set to ensure a realistic assessment of the model's generalization performance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pexhmBrRBgH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUESTION **4**"
      ],
      "metadata": {
        "id": "y46OpSN1Bhzs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Increasing the number of iterations in cross-validation can help mitigate some challenges associated with very small training or validation datasets. However, it's essential to be aware of certain considerations:\n",
        "\n",
        "Increased Robustness:\n",
        "\n",
        "With a small dataset, there might be a higher chance of variability in the performance metric due to the specific samples chosen for training and testing in each split. Increasing the number of iterations (higher k) allows the model to be trained and evaluated on different subsets, providing a more robust estimate of its performance.\n",
        "Better Representation:\n",
        "\n",
        "Each iteration in cross-validation involves using a different subset for validation. With a higher number of iterations, the model is exposed to a larger variety of training and validation samples, which can help it better capture the underlying patterns in the data.\n",
        "Optimal Use of Limited Data:\n",
        "\n",
        "In situations where the dataset is very small, you might want to use a higher fraction of the data for training in each fold to ensure that the model has sufficient information to learn. This can be achieved by using a smaller value of k (e.g., leave-one-out cross-validation or a small k-fold value).\n",
        "Despite these benefits, there are important considerations:\n",
        "\n",
        "Computational Cost:\n",
        "\n",
        "Increasing the number of iterations also increases the computational cost, as the model needs to be trained and evaluated more times. This is a trade-off that needs to be considered, especially if computational resources are limited.\n",
        "Limited Data for Training:\n",
        "\n",
        "With a very small dataset, each training set in a cross-validation fold may be extremely limited. This can make it challenging for the model to learn complex patterns, and the estimate of performance may still be subject to high variability.\n",
        "Careful Interpretation:\n",
        "\n",
        "While increasing iterations can improve robustness, it doesn't increase the amount of information in the dataset. Care should be taken in interpreting results, especially when dealing with extremely small datasets.\n",
        "In summary, increasing the number of iterations in cross-validation can be a helpful strategy when dealing with very small datasets, but it's important to strike a balance between gaining robustness in the estimate and the computational cost. Additionally, other techniques such as data augmentation or considering more advanced model architectures may be explored to make the most of limited data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VdGdZOuMB02E"
      }
    }
  ]
}